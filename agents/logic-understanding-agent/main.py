"""Logic Understanding Agent - Specialized for Marketplace Financial Analysis"""
import os
import logging
import time
import uuid
from datetime import datetime
from typing import Optional, Dict
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import vertexai
from vertexai.generative_models import GenerativeModel, GenerationConfig
import httpx
from google.cloud import secretmanager, firestore

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="Logic Understanding Agent - Marketplace Expert")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
PROJECT_ID = os.getenv("PROJECT_ID", "financial-reports-ai-2024")
LOCATION = os.getenv("REGION", "us-central1")
REPORT_READER_URL = os.getenv("REPORT_READER_URL", "https://report-reader-agent-38390150695.us-central1.run.app")

vertexai.init(project=PROJECT_ID, location=LOCATION)

# Firestore client for feedback storage
db = firestore.Client(project=PROJECT_ID)

# Gemini model with optimized config
generation_config = GenerationConfig(
    temperature=0.3,  # –ë–æ–ª–µ–µ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
    top_p=0.8,
    top_k=40,
    max_output_tokens=1024,  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –æ—Ç–≤–µ—Ç–∞
)

model = GenerativeModel(
    "gemini-2.0-flash-exp",
    generation_config=generation_config
)

# Default system instruction (fallback)
DEFAULT_SYSTEM_INSTRUCTION = """–¢—ã –æ–ø—ã—Ç–Ω—ã–π —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—â–∏–π—Å—è –Ω–∞ –∞–Ω–∞–ª–∏–∑–µ –æ—Ç—á–µ—Ç–æ–≤ –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å–æ–≤.

**–¢–≤–æ—è —Ä–æ–ª—å:**
- –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –æ—Ç—á–µ—Ç—ã —Å –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å–æ–≤ (–ø—Ä–æ–¥–∞–∂–∏, —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏, –º–µ—Ç—Ä–∏–∫–∏)
- –í—ã—è–≤–ª—è—Ç—å —Ç—Ä–µ–Ω–¥—ã, –∞–Ω–æ–º–∞–ª–∏–∏, –∫–ª—é—á–µ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏
- –î–∞–≤–∞—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –≤—ã–≤–æ–¥—ã –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

**–ü—Ä–∞–≤–∏–ª–∞ –æ–±—â–µ–Ω–∏—è:**
- –ë—É–¥—å –ö–†–ê–¢–ö–ò–ú –∏ –ø–æ —Å—É—â–µ—Å—Ç–≤—É (–º–∞–∫—Å–∏–º—É–º 3-4 –∞–±–∑–∞—Ü–∞)
- –ï—Å–ª–∏ –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ñ–∞–π–ª–∞ - –ø–æ–ø—Ä–æ—Å–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –æ—Ç—á–µ—Ç
- –§–æ–∫—É—Å–∏—Ä—É–π—Å—è –Ω–∞ –∫–ª—é—á–µ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫–∞—Ö: –≤—ã—Ä—É—á–∫–∞, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π, —Å—Ä–µ–¥–Ω–∏–π —á–µ–∫, –¥–∏–Ω–∞–º–∏–∫–∞
- –ù–µ —É—Ö–æ–¥–∏ –≤ –æ–±—â–∏–µ —Å–æ–≤–µ—Ç—ã - —Ä–∞–±–æ—Ç–∞–π —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
- –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ

**–ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç –æ–±—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã:**
–°–∫–∞–∂–∏: "–Ø —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—Å—å –Ω–∞ –∞–Ω–∞–ª–∏–∑–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –æ—Ç—á–µ—Ç–æ–≤ –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å–æ–≤. –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª —Å–ª–µ–≤–∞, –∏ —è –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É—é –≤–∞—à–∏ –¥–∞–Ω–Ω—ã–µ: –≤—ã—Ä—É—á–∫—É, —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏, —Ç—Ä–µ–Ω–¥—ã –ø—Ä–æ–¥–∞–∂."
"""

def get_system_prompt() -> str:
    """
    Load system prompt from Secret Manager.
    Falls back to DEFAULT_SYSTEM_INSTRUCTION if secret is unavailable.
    """
    try:
        client = secretmanager.SecretManagerServiceClient()
        secret_name = f"projects/{PROJECT_ID}/secrets/GEMINI_SYSTEM_PROMPT/versions/latest"
        
        logger.info(f"Attempting to load system prompt from Secret Manager: {secret_name}")
        
        response = client.access_secret_version(request={"name": secret_name})
        prompt = response.payload.data.decode("UTF-8")
        
        logger.info("‚úÖ System prompt loaded successfully from Secret Manager")
        return prompt
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Failed to load prompt from Secret Manager, using default: {e}")
        return DEFAULT_SYSTEM_INSTRUCTION

# Cache the system prompt on startup (refresh every request for dynamic updates)
# For production, you might want to cache this for a few minutes
_cached_prompt = None
_last_prompt_refresh = 0
PROMPT_CACHE_SECONDS = 60  # Refresh every 60 seconds

def get_cached_system_prompt() -> str:
    """Get system prompt with caching to avoid excessive Secret Manager calls"""
    global _cached_prompt, _last_prompt_refresh
    
    current_time = time.time()
    if _cached_prompt is None or (current_time - _last_prompt_refresh) > PROMPT_CACHE_SECONDS:
        _cached_prompt = get_system_prompt()
        _last_prompt_refresh = current_time
        logger.info("üîÑ System prompt cache refreshed")
    
    return _cached_prompt

# In-memory cache for request data (for regenerate functionality)
_request_cache: Dict[str, Dict] = {}

class AnalyzeRequest(BaseModel):
    query: str
    report_id: Optional[str] = None
    context: Optional[Dict] = None
    options: Optional[Dict] = None

class AnalyzeResponse(BaseModel):
    status: str
    insights: str
    request_id: str  # NEW: for feedback tracking
    agent_mode: str = "marketplace_expert"
    metadata: Dict = {}

class FeedbackRequest(BaseModel):
    request_id: str
    feedback_type: str  # "positive" or "negative"
    comment: Optional[str] = None

class RegenerateRequest(BaseModel):
    request_id: str

async def read_file_from_storage(file_path: str) -> Dict:
    """Read file using report-reader-agent"""
    try:
        endpoint = f"{REPORT_READER_URL}/read/storage"
        payload = {"request": {"file_path": file_path}}  # Nested structure for FastAPI
        
        logger.info(f"Reading file from storage: {file_path}")
        
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(endpoint, json=payload)
            
            if response.status_code == 200:
                logger.info(f"‚úÖ File read successfully: {file_path}")
                return response.json()
            else:
                logger.error(f"‚ùå Report reader failed: {response.status_code}")
                return {"error": f"Report reader failed: {response.status_code}"}
    
    except Exception as e:
        logger.error(f"‚ùå Failed to read file: {str(e)}")
        return {"error": f"Failed to read file: {str(e)}"}

@app.get("/health")
async def health():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "agent": "marketplace-financial-analyst",
        "model": "gemini-2.0-flash-exp",
        "specialization": "marketplace_reports",
        "features": ["dynamic_prompts", "secret_manager", "user_feedback", "regenerate"]
    }

@app.post("/analyze", response_model=AnalyzeResponse)
async def analyze_report(request: AnalyzeRequest):
    """AI analysis specialized for marketplace financial reports"""
    try:
        # Generate unique request_id
        request_id = str(uuid.uuid4())
        
        file_data = None
        data_summary = ""
        
        # Load dynamic system prompt
        system_instruction = get_cached_system_prompt()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ file_path –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
        if request.context and "file_path" in request.context:
            file_path = request.context["file_path"]
            
            # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª —á–µ—Ä–µ–∑ report-reader-agent
            file_result = await read_file_from_storage(file_path)
            
            if "error" not in file_result:
                file_data = file_result
                
                # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
                if "data" in file_result:
                    data_info = file_result["data"]
                    rows_count = data_info.get("rows", 0)
                    columns = data_info.get("columns", [])
                    sample_data = data_info.get("data", [])[:3]  # –ü–µ—Ä–≤—ã–µ 3 —Å—Ç—Ä–æ–∫–∏
                    
                    data_summary = f"""
**–ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π –æ—Ç—á–µ—Ç:**
–°—Ç—Ä–æ–∫: {rows_count}
–°—Ç–æ–ª–±—Ü—ã: {', '.join(columns[:15])}

–û–±—Ä–∞–∑–µ—Ü –¥–∞–Ω–Ω—ã—Ö (–ø–µ—Ä–≤—ã–µ 3 –∑–∞–ø–∏—Å–∏):
```
{chr(10).join([str(row) for row in sample_data])}
```
"""
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
        if data_summary:
            prompt = f"""{system_instruction}

**–î–ê–ù–ù–´–ï –ò–ó –û–¢–ß–ï–¢–ê:**
{data_summary}

**–í–û–ü–†–û–° –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø:**
{request.query}

**–¢–í–û–Ø –ó–ê–î–ê–ß–ê:**
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–∞–Ω–Ω—ã–µ –∏ –æ—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å. –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º, —Ñ–æ–∫—É—Å–∏—Ä—É–π—Å—è –Ω–∞ —Ü–∏—Ñ—Ä–∞—Ö –∏ —Ç—Ä–µ–Ω–¥–∞—Ö. –ú–∞–∫—Å–∏–º—É–º 4 –∞–±–∑–∞—Ü–∞.
"""
        else:
            # –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö - –∫–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç
            prompt = f"""{system_instruction}

–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç: "{request.query}"

–£ —Ç–µ–±—è –ù–ï–¢ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –û—Ç–≤–µ—Ç—å –∫—Ä–∞—Ç–∫–æ (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è) –∏ –ø–æ–ø—Ä–æ—Å–∏ –∑–∞–≥—Ä—É–∑–∏—Ç—å –æ—Ç—á–µ—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.
"""
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —Å retry logic –¥–ª—è 429 errors
        max_retries = 3
        retry_delay = 2
        
        for attempt in range(max_retries):
            try:
                logger.info(f"Generating AI response (attempt {attempt + 1}/{max_retries})")
                response = model.generate_content(prompt)
                logger.info("‚úÖ AI response generated successfully")
                break
            except Exception as gemini_error:
                if "429" in str(gemini_error) or "Resource exhausted" in str(gemini_error):
                    if attempt < max_retries - 1:
                        wait_time = retry_delay * (2 ** attempt)
                        logger.warning(f"‚ö†Ô∏è Rate limit hit, retrying in {wait_time}s...")
                        time.sleep(wait_time)
                        continue
                    else:
                        raise HTTPException(
                            status_code=429,
                            detail="–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤. –ü–æ–¥–æ–∂–¥–∏—Ç–µ 30 —Å–µ–∫—É–Ω–¥ –∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞."
                        )
                raise
        
        # Cache request for regenerate functionality
        _request_cache[request_id] = {
            "query": request.query,
            "context": request.context,
            "options": request.options,
            "prompt": prompt,
            "response": response.text,
            "timestamp": datetime.utcnow().isoformat()
        }
        
        return AnalyzeResponse(
            status="completed",
            insights=response.text,
            request_id=request_id,
            agent_mode="marketplace_expert",
            metadata={
                "model": "gemini-2.0-flash-exp",
                "has_file_data": file_data is not None,
                "rows_analyzed": file_data.get("data", {}).get("rows", 0) if file_data else 0,
                "prompt_source": "secret_manager"
            }
        )
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Analysis failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Analysis failed: {str(e)}")

@app.post("/feedback")
async def submit_feedback(request: FeedbackRequest):
    """Store user feedback in Firestore"""
    try:
        # Get cached request data
        cached_request = _request_cache.get(request.request_id)
        
        if not cached_request:
            raise HTTPException(
                status_code=404,
                detail="Request ID not found. Feedback can only be submitted for recent requests."
            )
        
        # Prepare feedback document
        feedback_data = {
            "request_id": request.request_id,
            "feedback_type": request.feedback_type,
            "comment": request.comment,
            "timestamp": datetime.utcnow(),
            "user_query": cached_request.get("query"),
            "ai_response": cached_request.get("response"),
            "prompt_used": cached_request.get("prompt")[:500],  # Truncate for storage
        }
        
        # Store in Firestore
        doc_ref = db.collection("feedback").document(request.request_id)
        doc_ref.set(feedback_data)
        
        logger.info(f"‚úÖ Feedback stored: {request.request_id} - {request.feedback_type}")
        
        return {
            "status": "success",
            "message": f"Feedback ({request.feedback_type}) stored successfully",
            "request_id": request.request_id
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Failed to store feedback: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to store feedback: {str(e)}")

@app.post("/regenerate", response_model=AnalyzeResponse)
async def regenerate_response(request: RegenerateRequest):
    """Regenerate AI response for a previous request"""
    try:
        # Get cached request data
        cached_request = _request_cache.get(request.request_id)
        
        if not cached_request:
            raise HTTPException(
                status_code=404,
                detail="Request ID not found. Can only regenerate recent requests."
            )
        
        # Generate new request_id for regenerated response
        new_request_id = str(uuid.uuid4())
        
        logger.info(f"Regenerating response for request: {request.request_id}")
        
        # Use the same prompt but generate new response
        prompt = cached_request.get("prompt")
        
        # Generate new response
        max_retries = 3
        retry_delay = 2
        
        for attempt in range(max_retries):
            try:
                logger.info(f"Generating regenerated response (attempt {attempt + 1}/{max_retries})")
                response = model.generate_content(prompt)
                logger.info("‚úÖ Regenerated response generated successfully")
                break
            except Exception as gemini_error:
                if "429" in str(gemini_error) or "Resource exhausted" in str(gemini_error):
                    if attempt < max_retries - 1:
                        wait_time = retry_delay * (2 ** attempt)
                        logger.warning(f"‚ö†Ô∏è Rate limit hit, retrying in {wait_time}s...")
                        time.sleep(wait_time)
                        continue
                    else:
                        raise HTTPException(
                            status_code=429,
                            detail="–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤. –ü–æ–¥–æ–∂–¥–∏—Ç–µ 30 —Å–µ–∫—É–Ω–¥ –∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞."
                        )
                raise
        
        # Cache new regenerated request
        _request_cache[new_request_id] = {
            "query": cached_request.get("query"),
            "context": cached_request.get("context"),
            "options": cached_request.get("options"),
            "prompt": prompt,
            "response": response.text,
            "timestamp": datetime.utcnow().isoformat(),
            "regenerated_from": request.request_id
        }
        
        return AnalyzeResponse(
            status="completed",
            insights=response.text,
            request_id=new_request_id,
            agent_mode="marketplace_expert",
            metadata={
                "model": "gemini-2.0-flash-exp",
                "regenerated": True,
                "original_request_id": request.request_id,
                "prompt_source": "secret_manager"
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Regenerate failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Regenerate failed: {str(e)}")

@app.get("/test-connection")
async def test_report_reader():
    """Test connection to report-reader-agent"""
    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get(f"{REPORT_READER_URL}/health")
            return {
                "report_reader_status": response.status_code,
                "report_reader_url": REPORT_READER_URL,
                "response": response.json() if response.status_code == 200 else response.text
            }
    except Exception as e:
        return {
            "error": str(e),
            "report_reader_url": REPORT_READER_URL
        }

@app.get("/prompt/info")
async def get_prompt_info():
    """Get information about current system prompt (for debugging)"""
    try:
        current_prompt = get_cached_system_prompt()
        return {
            "status": "success",
            "prompt_length": len(current_prompt),
            "prompt_source": "secret_manager" if current_prompt != DEFAULT_SYSTEM_INSTRUCTION else "default",
            "cache_age_seconds": time.time() - _last_prompt_refresh,
            "prompt_preview": current_prompt[:200] + "..." if len(current_prompt) > 200 else current_prompt
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e)
        }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8080)
